---------------------------爬虫的流程

1. 设定抓取目标（种子页面/起始页面）并获取网页。
2. 当服务器无法访问时，按照指定的重试次数尝试重新下载页面。
3. 在需要的时候设置用户代理或隐藏真实IP，否则可能无法访问页面。
4. 对获取的页面进行必要的解码操作然后抓取出需要的信息。
5. 在获取的页面中通过某种方式（如正则表达式）抽取出页面中的链接信息。
6. 对链接进行进一步的处理（获取页面并重复上面的动作）。
7. 将有用的信息进行持久化以备后续的处理。



---------------------------爬虫涉及到的标准库和第三方库

1. 下载数据 - urllib / requests / aiohttp。
2. 解析数据 - re / lxml / beautifulsoup4 / pyquery。
3. 缓存和持久化 - pymysql / sqlalchemy / peewee/ redis / pymongo。
4. 生成数字签名 - hashlib。
5. 序列化和压缩 - pickle / json / zlib。
6. 调度器 - 多进程（multiprocessing） / 多线程（threading）。



--------------------------解析网页内容的知识
        正则表达式\\ bs4\\ xpath\\ jsonpath



--------------------------涉及到的动态HTML
        selenium + phantomjs / chromeheadless



--------------------------scrapy框架，scrapy-redis组件
